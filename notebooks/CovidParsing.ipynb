{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "import psycopg2\n",
    "import pickle\n",
    "import requests\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect():\n",
    "    conn = psycopg2.connect(\n",
    "        user=\"postgres\",\n",
    "        password=os.environ.get(\"POSTGRES_PASS\", \"\"),\n",
    "        host=\"localhost\",\n",
    "        port=5432,\n",
    "        database=\"venmo\",\n",
    "    )\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_WORDS = [\n",
    "    # Machine Learning to Detect Self-Reporting of Symptoms, Testing Access, and Recovery Associated With COVID-19 on Twitter\n",
    "    \"covid\",\n",
    "    \"diagnosed\",\n",
    "    \"pneumonia\",\n",
    "    \"coronavirus\",\n",
    "    \"fever\",\n",
    "    \"test\",\n",
    "    \"symptoms\",\n",
    "    \"isolating\",\n",
    "    \"cough\",\n",
    "    \"emergency room\",\n",
    "    # Extras\n",
    "    \"isolating\",\n",
    "    \"quarantine\",\n",
    "    \"sick\",\n",
    "    \"social distancing\",\n",
    "    \"self isolat\",\n",
    "    \"mask\",\n",
    "]\n",
    "with open(\"covid_words.pkl\", \"wb\") as f:\n",
    "    pickle.dump(COVID_WORDS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta = Counter()\n",
    "\n",
    "# Store timestamps\n",
    "covid_token_usage = {}\n",
    "covid_token_refs = {}\n",
    "covid_token_examples = {}\n",
    "for w in COVID_WORDS:\n",
    "    covid_token_usage[w] = []\n",
    "    covid_token_examples[w] = []\n",
    "    covid_token_refs[w] = Counter()\n",
    "\n",
    "conn = connect()\n",
    "with conn.cursor(name=\"covid_exploration\") as cursor:\n",
    "    cursor.itersize = 2000\n",
    "    cursor.execute(\"SELECT * FROM transactions\")\n",
    "    for i, row in enumerate(cursor):\n",
    "\n",
    "        if i % 2_000_000 == 0:\n",
    "            # checkpoint\n",
    "            print(\"Row\", i)\n",
    "            with open(\"covid_tokens.pkl\", \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    (covid_token_usage, covid_token_refs, covid_token_examples), f\n",
    "                )\n",
    "            with open(\"covid_meta.pkl\", \"wb\") as f:\n",
    "                pickle.dump(meta, f)\n",
    "\n",
    "        msg = row[1]\n",
    "        try:\n",
    "            msg = re.sub(r\"[^\\w\\d_\\- ]\", \"\", msg).strip().replace(\"-\", \" \")\n",
    "            meta[\"msgs\"] += 1\n",
    "            if len(msg) == 0:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "        meta[\"msgs_processed\"] += 1\n",
    "\n",
    "        try:\n",
    "            ts = int(row[4].timestamp())\n",
    "        except:\n",
    "            continue\n",
    "        meta[\"msgs_ts_processed\"] += 1\n",
    "\n",
    "        for token in COVID_WORDS:\n",
    "            if token in msg:\n",
    "                meta[\"covid_tokens_found\"] += 1\n",
    "                covid_token_usage[token].append(ts)\n",
    "                covid_token_examples[token].append(row[1])\n",
    "                refs = covid_token_refs[token]\n",
    "                for word in msg.split(\" \"):\n",
    "                    refs[word] += 1\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "with open(\"covid_tokens.pkl\", \"rb\") as f:\n",
    "    (\n",
    "        covid_token_usage_saved,\n",
    "        covid_token_refs_saved,\n",
    "        covid_token_examples_saved,\n",
    "    ) = pickle.load(f)\n",
    "with open(\"covid_meta.pkl\", \"rb\") as f:\n",
    "    meta_saved = pickle.load(f)\n",
    "meta_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 71%\n",
    "meta_saved[\"msgs_ts_processed\"] / meta_saved[\"msgs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.016%\n",
    "meta_saved[\"covid_tokens_found\"] / meta_saved[\"msgs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing too interesting here\n",
    "for token, cntr in covid_token_refs_saved.items():\n",
    "    print(token)\n",
    "    print(cntr.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = {\"token\": [], \"Date\": []}\n",
    "for token, usage_ts in covid_token_usage_saved.items():\n",
    "    for ts in usage_ts:\n",
    "        df_data[\"token\"].append(token)\n",
    "        df_data[\"Date\"].append(ts)\n",
    "df = pd.DataFrame(df_data)\n",
    "# Start at the end of 2019\n",
    "df = df[df[\"Date\"] > 1575158400]\n",
    "df = df[df[\"Date\"] < 1602720000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.read_csv(\"United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\")\n",
    "cases_df = cases_df[[\"submission_date\", \"new_case\"]]\n",
    "cases_df = cases_df.groupby(\"submission_date\").sum()\n",
    "cases_df[\"US Daily Cases\"] = cases_df.new_case.rolling(7).mean()\n",
    "cases_df = cases_df.dropna().reset_index()\n",
    "cases_df[\"Date\"] = cases_df.submission_date.apply(\n",
    "    lambda date: datetime.datetime.strptime(date, \"%m/%d/%Y\").timestamp()\n",
    ")\n",
    "cases_df = cases_df[cases_df[\"Date\"] < 1602720000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
    "fig.suptitle(\"COVID-19 Tokens In Transactions\", fontsize=16)\n",
    "sns.histplot(\n",
    "    df[df[\"token\"].isin([\"covid\", \"coronavirus\"])],\n",
    "    x=\"Date\",\n",
    "    hue=\"token\",\n",
    "    ax=ax1,\n",
    ")\n",
    "sns.histplot(\n",
    "    df[df[\"token\"].isin([\"quarantine\"])],\n",
    "    x=\"Date\",\n",
    "    hue=\"token\",\n",
    "    ax=ax2,\n",
    ")\n",
    "sns.histplot(\n",
    "    df[df[\"token\"].isin([\"cough\", \"pneumonia\", \"fever\", \"symptoms\"])],\n",
    "    x=\"Date\",\n",
    "    hue=\"token\",\n",
    "    ax=ax3,\n",
    ")\n",
    "sns.histplot(\n",
    "    df[df[\"token\"].isin([\"self isolat\", \"isolating\", \"social distancing\"])],\n",
    "    x=\"Date\",\n",
    "    hue=\"token\",\n",
    "    ax=ax4,\n",
    ")\n",
    "ticks = ax1.get_xticks()\n",
    "for a in [ax1, ax2, ax3, ax4]:\n",
    "    a.set_xticks(ticks)\n",
    "    a.set_xticklabels(\n",
    "        [datetime.datetime.fromtimestamp(ts).isoformat()[:10] for ts in ticks]\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        data=cases_df,\n",
    "        x=\"Date\",\n",
    "        y=\"US Daily Cases\",\n",
    "        ax=a.twinx(),\n",
    "        color=\"red\",\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS_OF_INTEREST = [\"social distancing\", \"quarantine\", \"covid\", \"cough\"]\n",
    "K = 5\n",
    "df_data = {\"phrase\": [], \"examples\": []}\n",
    "for token in TOKENS_OF_INTEREST:\n",
    "    if len(covid_token_examples_saved[token]) < K:\n",
    "        continue\n",
    "    sample_usage = random.sample(covid_token_examples_saved[token], K)\n",
    "    df_data[\"phrase\"].append(token)\n",
    "    df_data[\"examples\"].append(\"\\n\".join(sample_usage))\n",
    "df = pd.DataFrame(df_data)\n",
    "df.set_index(\"phrase\")\n",
    "\n",
    "\n",
    "def pretty_print(df):\n",
    "    return display(HTML(df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
    "\n",
    "\n",
    "pretty_print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
