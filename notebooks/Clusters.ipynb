{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import psycopg2\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect():\n",
    "    conn = psycopg2.connect(\n",
    "        user=\"postgres\",\n",
    "        password=os.environ.get(\"POSTGRES_PASS\", \"\"),\n",
    "        host=\"localhost\",\n",
    "        port=5432,\n",
    "        database=\"venmo\",\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "\n",
    "def reduce_graph(old_graph, f):\n",
    "    new_graph = nx.Graph()\n",
    "    for a, b, data in old_graph.edges(data=True):\n",
    "        if f(data):\n",
    "            new_graph.add_edge(a, b, **data)\n",
    "    return new_graph\n",
    "\n",
    "\n",
    "def find_coords(place, cache):\n",
    "    found = cache.get(place)\n",
    "    if found:\n",
    "        return (found.latitude, found.longitude)\n",
    "    location = geolocator.geocode(place)\n",
    "    if location is None:\n",
    "        return None\n",
    "    cache[place] = location\n",
    "    return (location.latitude, location.longitude)\n",
    "\n",
    "\n",
    "def parse_geo_tokens(geoparser, raw_msg, cache):\n",
    "    msg = re.sub(r\"[^\\w\\d_\\- ]\", \"\", raw_msg).strip()\n",
    "    if len(msg) == 0:\n",
    "        return []\n",
    "    found = cache.get(msg)\n",
    "    if found:\n",
    "        return found\n",
    "    result = geoparser.geoparse(msg)\n",
    "    cache[msg] = result\n",
    "    return result\n",
    "\n",
    "\n",
    "with open(\"user_id_to_loc.pkl\", \"rb\") as f:\n",
    "    user_id_to_loc_saved = pickle.load(f)\n",
    "with open(\"geo_cache.pkl\", \"rb\") as f:\n",
    "    geo_cache = pickle.load(f)\n",
    "known_user_ids = set(user_id_to_loc_saved)\n",
    "with open(\"covid_words.pkl\", \"rb\") as f:\n",
    "    covid_words = pickle.load(f)\n",
    "geoparser_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    LEAST(actor_user_id, recipient_id),\n",
    "    GREATEST(actor_user_id, recipient_id),\n",
    "    array_agg(id),\n",
    "    array_agg(created),\n",
    "    array_agg(message)\n",
    "FROM \n",
    "    transactions\n",
    "WHERE \n",
    "    created > '2020-02-01'\n",
    "GROUP BY \n",
    "    GREATEST(actor_user_id, recipient_id),\n",
    "    LEAST(actor_user_id, recipient_id)\n",
    "\"\"\"\n",
    "\n",
    "conn = connect()\n",
    "with conn.cursor(name=\"clusters\") as cursor:\n",
    "    cursor.itersize = 500\n",
    "    cursor.execute(query)\n",
    "    for i, (a, b, ids, createds, msgs) in enumerate(cursor):\n",
    "        if i % 1_000_000 == 0 and i != 0:\n",
    "            print(\"Checkpoint @\", i)\n",
    "            with open(\"cluster_graph.pkl\", \"wb\") as f:\n",
    "                pickle.dump(graph, f)\n",
    "        graph.add_edge(a, b, weight=len(ids), dates=createds, msgs=msgs)\n",
    "\n",
    "with open(\"cluster_graph.pkl\", \"wb\") as f:\n",
    "    pickle.dump(graph, f)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cluster_graph.pkl\", \"rb\") as f:\n",
    "    graph_saved = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_edge(data):\n",
    "    return len(data[\"msgs\"]) >= 4\n",
    "\n",
    "\n",
    "rgraph = reduce_graph(graph_saved, filter_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graphs = nx.connected_components(rgraph)\n",
    "cluster_df_data = defaultdict(list)\n",
    "\n",
    "for i, sg in enumerate(sub_graphs):\n",
    "\n",
    "    if i % 100_000 == 0:\n",
    "        print(\"@ Subgraph\", i)\n",
    "\n",
    "    known_loc_overlap = sg & known_user_ids\n",
    "    cluster = rgraph.subgraph(sg)\n",
    "\n",
    "    msgs = []\n",
    "    dates = []\n",
    "    covid_cnt = 0\n",
    "    edges = 0\n",
    "    transactions = 0\n",
    "    for _, _, edge_msgs in cluster.edges.data(\"msgs\"):\n",
    "        edges += 1\n",
    "        for m in edge_msgs:\n",
    "            transactions += 1\n",
    "            for token in covid_words:\n",
    "                if token in m:\n",
    "                    covid_cnt += 1\n",
    "        msgs.extend(edge_msgs)\n",
    "    for _, _, edge_dates in cluster.edges.data(\"dates\"):\n",
    "        dates.extend(edge_dates)\n",
    "\n",
    "    cluster_df_data[\"size\"].append(len(sg))\n",
    "    cluster_df_data[\"edges_cnt\"].append(edges)\n",
    "    cluster_df_data[\"transactions_cnt\"].append(transactions)\n",
    "    cluster_df_data[\"covid_cnt\"].append(covid_cnt)\n",
    "    cluster_df_data[\"msgs\"].append(msgs)\n",
    "    cluster_df_data[\"dates\"].append(dates)\n",
    "    cluster_df_data[\"known_overlap\"].append(len(known_loc_overlap))\n",
    "    cluster_df_data[\"known_overlap_ids\"].append(list(known_loc_overlap))\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_df_data)\n",
    "with open(\"cluster_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cluster_df_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mordecai import Geoparser\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "with open(\"cluster_data.pkl\", \"rb\") as f:\n",
    "    cluster_df = pd.DataFrame(pickle.load(f))\n",
    "with open(\"user_id_to_loc.pkl\", \"rb\") as f:\n",
    "    user_id_to_loc_saved = pickle.load(f)\n",
    "\n",
    "geo = Geoparser()\n",
    "geolocator = Nominatim(user_agent=\"sshh12/venmo-research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idxs = []\n",
    "lats = []\n",
    "lngs = []\n",
    "place_tokens = []\n",
    "\n",
    "\n",
    "def save_checkpoint():\n",
    "    print(\"Saving...\")\n",
    "    with open(\"cluster_locs_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump((idxs, lats, lngs, place_tokens), f)\n",
    "    with open(\"geo_cache.pkl\", \"wb\") as f:\n",
    "        pickle.dump(geo_cache, f)\n",
    "    with open(\"geoparser_cache.pkl\", \"wb\") as f:\n",
    "        pickle.dump(geoparser_cache, f)\n",
    "\n",
    "\n",
    "for idx, row in cluster_df.iterrows():\n",
    "    size = row[\"size\"]\n",
    "    msgs = row[\"msgs\"]\n",
    "    known_ids = row[\"known_overlap_ids\"]\n",
    "    lat, lng, ptokens = None, None, []\n",
    "    if len(known_ids) > 0:\n",
    "        assume_loc = [user_id_to_loc_saved[u] for u in known_ids][0]\n",
    "        lat, lng, _ = assume_loc\n",
    "    else:\n",
    "        if size < 100:\n",
    "            locs = []\n",
    "            for m in msgs:\n",
    "                locs.extend(parse_geo_tokens(geo, m, geoparser_cache))\n",
    "            places = [\n",
    "                (item[\"word\"], item[\"geo\"][\"admin1\"]) for item in locs if \"geo\" in item\n",
    "            ]\n",
    "            ptokens.extend([p[0] for p in places])\n",
    "            c = Counter([p[1] for p in places])\n",
    "            if len(places) > 0:\n",
    "                loc_name = c.most_common(1)[0][0]\n",
    "                loc_coords = find_coords(loc_name, geo_cache)\n",
    "                if loc_coords is not None:\n",
    "                    lat, lng = loc_coords\n",
    "    lats.append(lat)\n",
    "    lngs.append(lng)\n",
    "    place_tokens.append(ptokens)\n",
    "    idxs.append(idx)\n",
    "    if idx % 10000 == 0 and idx > 0:\n",
    "        save_checkpoint()\n",
    "\n",
    "save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cluster_locs_data.pkl\", \"rb\") as f:\n",
    "    idxs, lats, lngs, place_tokens = pickle.load(f)\n",
    "with open(\"cluster_data.pkl\", \"rb\") as f:\n",
    "    cluster_df = pd.DataFrame(pickle.load(f)).iloc[idxs]\n",
    "with open(\"user_id_to_loc.pkl\", \"rb\") as f:\n",
    "    user_id_to_loc_saved = pickle.load(f)\n",
    "cluster_df[\"lat\"] = lats\n",
    "cluster_df[\"lng\"] = lngs\n",
    "cluster_df[\"place_tokens\"] = place_tokens\n",
    "cluster_df_clean = cluster_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Total Clusters: 902451\n",
    "Clusters w/latlng: 78613 8.711054672220431 %\n",
    "Mean cluster size: 10.188238586493329\n",
    "\"\"\"\n",
    "print(\"Total Clusters:\", len(cluster_df))\n",
    "print(\n",
    "    \"Clusters w/latlng:\",\n",
    "    len(cluster_df_clean),\n",
    "    len(cluster_df_clean) / len(cluster_df) * 100,\n",
    "    \"%\",\n",
    ")\n",
    "print(\"Mean cluster size:\", cluster_df_clean[\"size\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df_clean[\"size\"].clip(0, 80).plot.hist(bins=80, title=\"Group Sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df_clean[\"covid_cnt\"].clip(0, 4).plot.hist(\n",
    "    title=\"COVID Token Counts Per Group\", xticks=[0, 1, 2, 3, 4, 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_TO_ABBR = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virgin Islands\": \"VI\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import geoplot.crs as gcrs\n",
    "import geoplot as gplt\n",
    "import geopandas as gpd\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "contiguous_usa = gpd.read_file(gplt.datasets.get_path(\"contiguous_usa\"))\n",
    "usa = world[world.name == \"United States of America\"]\n",
    "\n",
    "cases_df = pd.read_csv(\"United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\")\n",
    "cases_df[\"Date\"] = cases_df.submission_date.apply(\n",
    "    lambda date: datetime.datetime.strptime(date, \"%m/%d/%Y\").timestamp()\n",
    ")\n",
    "cases_df = cases_df[cases_df.Date < 1602720000]\n",
    "total_cases = cases_df.groupby(\"state\")[[\"tot_cases\"]].last()\n",
    "contiguous_usa[\"abbr\"] = contiguous_usa.state.apply(STATE_TO_ABBR.__getitem__)\n",
    "contiguous_usa[\"covid_cases\"] = contiguous_usa.abbr.apply(\n",
    "    lambda a: total_cases.loc[a][\"tot_cases\"]\n",
    ")\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    cluster_df_clean,\n",
    "    geometry=gpd.points_from_xy(cluster_df_clean.lng, cluster_df_clean.lat),\n",
    ")\n",
    "usa_gdf = gdf.loc[gdf.within(usa.iloc[0][\"geometry\"])].copy()\n",
    "\n",
    "regions = len(contiguous_usa)\n",
    "cluster_cnts = [0 for _ in range(regions)]\n",
    "covid_cnts = [0 for _ in range(regions)]\n",
    "sum_size = [0 for _ in range(regions)]\n",
    "sum_trans = [0 for _ in range(regions)]\n",
    "trans_dates = [[] for _ in range(regions)]\n",
    "verified_trans_dates = [[] for _ in range(regions)]\n",
    "for idx, row in usa_gdf.iterrows():\n",
    "    if row[\"size\"] > 1000:\n",
    "        continue\n",
    "    state_match = None\n",
    "    for state_idx, state in contiguous_usa.iterrows():\n",
    "        if state[\"geometry\"].contains(Point(row[\"lng\"], row[\"lat\"])):\n",
    "            state_match = state_idx\n",
    "            break\n",
    "    if state_match is None:\n",
    "        continue\n",
    "    cluster_cnts[state_match] += 1\n",
    "    covid_cnts[state_match] += row[\"covid_cnt\"]\n",
    "    sum_size[state_match] += row[\"size\"]\n",
    "    sum_trans[state_match] += row[\"transactions_cnt\"]\n",
    "    trans_dates[state_match].extend(row[\"dates\"])\n",
    "    if row[\"known_overlap\"] > 0:\n",
    "        verified_trans_dates[state_match].extend(row[\"dates\"])\n",
    "\n",
    "contiguous_usa[\"cluster_cnts\"] = cluster_cnts\n",
    "contiguous_usa[\"covid_cnts\"] = covid_cnts\n",
    "contiguous_usa[\"sum_size\"] = sum_size\n",
    "contiguous_usa[\"sum_trans\"] = sum_trans\n",
    "contiguous_usa[\"trans_dates\"] = trans_dates\n",
    "contiguous_usa[\"verified_trans_dates\"] = verified_trans_dates\n",
    "\n",
    "cluster_trans_by_state = defaultdict(list)\n",
    "for idx, row in contiguous_usa.iterrows():\n",
    "    cluster_trans_by_state[row[\"state\"]].extend(row[\"verified_trans_dates\"])\n",
    "with open(\"transactions_by_state_cluster.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cluster_trans_by_state, f)\n",
    "\n",
    "print(round(len(usa_gdf) / len(gdf) * 100), \"% in USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contiguous_usa[\"avg_clust_size\"] = (\n",
    "    contiguous_usa[\"sum_size\"] / contiguous_usa[\"cluster_cnts\"]\n",
    ")\n",
    "contiguous_usa[\"avg_clust_trans\"] = (\n",
    "    contiguous_usa[\"sum_trans\"] / contiguous_usa[\"cluster_cnts\"]\n",
    ")\n",
    "contiguous_usa[\"covidtok_per_cluster\"] = (\n",
    "    contiguous_usa[\"covid_cnts\"] / contiguous_usa[\"cluster_cnts\"]\n",
    ")\n",
    "contiguous_usa[\"covidtok_per_pop\"] = (\n",
    "    contiguous_usa[\"covid_cnts\"] / contiguous_usa[\"population\"]\n",
    ")\n",
    "contiguous_usa[\"cases_per_pop\"] = (\n",
    "    contiguous_usa[\"covid_cases\"] / contiguous_usa[\"population\"]\n",
    ")\n",
    "contiguous_usa[\"clusters_per_pop\"] = (\n",
    "    contiguous_usa[\"cluster_cnts\"] / contiguous_usa[\"population\"]\n",
    ")\n",
    "contiguous_usa[\"covidtok_per_cases\"] = (\n",
    "    contiguous_usa[\"covid_cnts\"] / contiguous_usa[\"covid_cases\"]\n",
    ")\n",
    "state_corr = contiguous_usa.corr()\n",
    "state_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gplt.webmap(contiguous_usa, projection=gcrs.WebMercator(), figsize=(16, 16))\n",
    "gplt.pointplot(usa_gdf, ax=ax).set_title(\"Venmo Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplt.choropleth(\n",
    "    contiguous_usa,\n",
    "    hue=\"covid_cases\",\n",
    "    projection=gcrs.AlbersEqualArea(),\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    cmap=\"Purples\",\n",
    "    legend=True,\n",
    "    scheme=\"FisherJenks\",\n",
    "    legend_kwargs={\"loc\": \"lower left\"},\n",
    ").set_title(\"Cumulative COVID Cases\")\n",
    "gplt.choropleth(\n",
    "    contiguous_usa,\n",
    "    hue=\"covid_cnts\",\n",
    "    projection=gcrs.AlbersEqualArea(),\n",
    "    linewidth=1,\n",
    "    cmap=\"Greens\",\n",
    "    legend=True,\n",
    "    scheme=\"FisherJenks\",\n",
    "    legend_kwargs={\"loc\": \"lower left\"},\n",
    ").set_title(\"Cumulative COVID Keyword Mentions\")\n",
    "gplt.choropleth(\n",
    "    contiguous_usa,\n",
    "    hue=\"avg_clust_size\",\n",
    "    projection=gcrs.AlbersEqualArea(),\n",
    "    linewidth=1,\n",
    "    cmap=\"Blues\",\n",
    "    legend=True,\n",
    "    scheme=\"FisherJenks\",\n",
    "    legend_kwargs={\"loc\": \"lower left\"},\n",
    ").set_title(\"Mean Group Size\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
