{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import psycopg2\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect():\n",
    "    conn = psycopg2.connect(\n",
    "        user=\"postgres\",\n",
    "        password=os.environ.get(\"POSTGRES_PASS\", \"\"),\n",
    "        host=\"localhost\",\n",
    "        port=5432,\n",
    "        database=\"venmo\",\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "\n",
    "def reduce_graph(old_graph, f):\n",
    "    new_graph = nx.Graph()\n",
    "    for a, b, data in old_graph.edges(data=True):\n",
    "        if f(data):\n",
    "            new_graph.add_edge(a, b, **data)\n",
    "    return new_graph\n",
    "\n",
    "\n",
    "def find_coords(place, cache):\n",
    "    found = cache.get(place)\n",
    "    if found:\n",
    "        return (found.latitude, found.longitude)\n",
    "    location = geolocator.geocode(place)\n",
    "    if location is None:\n",
    "        return None\n",
    "    cache[place] = location\n",
    "    return (location.latitude, location.longitude)\n",
    "\n",
    "\n",
    "def parse_geo_tokens(geoparser, raw_msg):\n",
    "    msg = re.sub(r\"[^\\w\\d_\\- ]\", \"\", raw_msg).strip()\n",
    "    if len(msg) == 0:\n",
    "        return []\n",
    "    return geoparser.geoparse(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    LEAST(actor_user_id, recipient_id),\n",
    "    GREATEST(actor_user_id, recipient_id),\n",
    "    array_agg(id),\n",
    "    array_agg(created),\n",
    "    array_agg(message)\n",
    "FROM \n",
    "    transactions\n",
    "WHERE \n",
    "    created > '2020-03-10'\n",
    "GROUP BY \n",
    "    GREATEST(actor_user_id, recipient_id),\n",
    "    LEAST(actor_user_id, recipient_id)\n",
    "\"\"\"\n",
    "\n",
    "conn = connect()\n",
    "with conn.cursor(name=\"clusters\") as cursor:\n",
    "    cursor.itersize = 500\n",
    "    cursor.execute(query)\n",
    "    for i, (a, b, ids, createds, msgs) in enumerate(cursor):\n",
    "        if i % 1_000_000 == 0 and i != 0:\n",
    "            print(\"Checkpoint @\", i)\n",
    "            with open(\"cluster_graph.pkl\", \"wb\") as f:\n",
    "                pickle.dump(graph, f)\n",
    "        graph.add_edge(a, b, weight=len(ids), dates=createds, msgs=msgs)\n",
    "\n",
    "with open(\"cluster_graph.pkl\", \"wb\") as f:\n",
    "    pickle.dump(graph, f)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cluster_graph.pkl\", \"rb\") as f:\n",
    "    graph_saved = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"user_id_to_loc.pkl\", \"rb\") as f:\n",
    "    user_id_to_loc_saved = pickle.load(f)\n",
    "with open(\"geo_cache.pkl\", \"rb\") as f:\n",
    "    geo_cache = pickle.load(f)\n",
    "known_user_ids = set(user_id_to_loc_saved)\n",
    "\n",
    "COVID_WORDS = [\n",
    "    \"diagnosed\",\n",
    "    \"pneumonia\",\n",
    "    \"coronavirus\",\n",
    "    \"fever\",\n",
    "    \"covid\",\n",
    "    \"isolating\",\n",
    "    \"quarantine\",\n",
    "    \"cough\",\n",
    "    \"sick\",\n",
    "    \"social distancing\",\n",
    "    \"self isolat\",\n",
    "    \"self-isolat\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_edge(data):\n",
    "    return len(data[\"msgs\"]) >= 4\n",
    "\n",
    "\n",
    "rgraph = reduce_graph(graph_saved, filter_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graphs = nx.connected_components(rgraph)\n",
    "cluster_df_data = defaultdict(list)\n",
    "\n",
    "for i, sg in enumerate(sub_graphs):\n",
    "\n",
    "    if i % 100_000 == 0:\n",
    "        print(\"@ Subgraph\", i)\n",
    "\n",
    "    known_loc_overlap = sg & known_user_ids\n",
    "    cluster = rgraph.subgraph(sg)\n",
    "\n",
    "    msgs = []\n",
    "    covid_cnt = 0\n",
    "    edges = 0\n",
    "    transactions = 0\n",
    "    for _, _, edge_msgs in cluster.edges.data(\"msgs\"):\n",
    "        edges += 1\n",
    "        for m in edge_msgs:\n",
    "            transactions += 1\n",
    "            for token in COVID_WORDS:\n",
    "                if token in m:\n",
    "                    covid_cnt += 1\n",
    "        msgs.extend(edge_msgs)\n",
    "\n",
    "    cluster_df_data[\"size\"].append(len(sg))\n",
    "    cluster_df_data[\"edges_cnt\"].append(edges)\n",
    "    cluster_df_data[\"transactions_cnt\"].append(transactions)\n",
    "    cluster_df_data[\"covid_cnt\"].append(covid_cnt)\n",
    "    cluster_df_data[\"msgs\"].append(msgs)\n",
    "    cluster_df_data[\"known_overlap\"].append(len(known_loc_overlap))\n",
    "    cluster_df_data[\"known_overlap_ids\"].append(list(known_loc_overlap))\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_df_data)\n",
    "with open(\"cluster_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cluster_df_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mordecai import Geoparser\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "with open(\"cluster_data.pkl\", \"rb\") as f:\n",
    "    cluster_df = pd.DataFrame(pickle.load(f))\n",
    "with open(\"user_id_to_loc.pkl\", \"rb\") as f:\n",
    "    user_id_to_loc_saved = pickle.load(f)\n",
    "\n",
    "geo = Geoparser()\n",
    "geolocator = Nominatim(user_agent=\"sshh12/venmo-research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lats = []\n",
    "lngs = []\n",
    "place_tokens = []\n",
    "for idx, row in cluster_df.iterrows():\n",
    "    size = row[\"size\"]\n",
    "    msgs = row[\"msgs\"]\n",
    "    known_ids = row[\"known_overlap_ids\"]\n",
    "    lat, lng, ptokens = None, None, []\n",
    "    if len(known_ids) > 0:\n",
    "        assume_loc = [user_id_to_loc_saved[u] for u in known_ids][0]\n",
    "        lat, lng, _ = assume_loc\n",
    "    else:\n",
    "        if size < 100:\n",
    "            locs = []\n",
    "            for m in msgs:\n",
    "                locs.extend(parse_geo_tokens(geo, m))\n",
    "            places = [\n",
    "                (item[\"word\"], item[\"geo\"][\"admin1\"]) for item in locs if \"geo\" in item\n",
    "            ]\n",
    "            ptokens.extend([p[0] for p in places])\n",
    "            c = Counter([p[1] for p in places])\n",
    "            if len(places) > 0:\n",
    "                loc_name = c.most_common(1)[0][0]\n",
    "                loc_coords = find_coords(loc_name, geo_cache)\n",
    "                if loc_coords is not None:\n",
    "                    lat, lng = loc_coords\n",
    "    lats.append(lat)\n",
    "    lngs.append(lng)\n",
    "    place_tokens.append(ptokens)\n",
    "\n",
    "with open(\"cluster_locs_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump((lats, lngs, place_tokens), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cluster_data.pkl\", \"rb\") as f:\n",
    "    cluster_df = pd.DataFrame(pickle.load(f))\n",
    "with open(\"cluster_locs_data.pkl\", \"rb\") as f:\n",
    "    lats, lngs, place_tokens = pickle.load(f)\n",
    "with open(\"user_id_to_loc.pkl\", \"rb\") as f:\n",
    "    user_id_to_loc_saved = pickle.load(f)\n",
    "cluster_df[\"lat\"] = lats\n",
    "cluster_df[\"lng\"] = lngs\n",
    "cluster_df[\"place_tokens\"] = place_tokens\n",
    "cluster_df_clean = cluster_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    len(cluster_df_clean),\n",
    "    len(cluster_df),\n",
    "    len(cluster_df_clean) / len(cluster_df) * 100,\n",
    ")\n",
    "print(cluster_df[\"size\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_TO_ABBR = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import geoplot.crs as gcrs\n",
    "import geoplot as gplt\n",
    "import geopandas as gpd\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "contiguous_usa = gpd.read_file(gplt.datasets.get_path(\"contiguous_usa\"))\n",
    "usa = world[world.name == \"United States of America\"]\n",
    "\n",
    "total_cases = (\n",
    "    pd.read_csv(\"United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv\")\n",
    "    .groupby(\"state\")[[\"tot_cases\"]]\n",
    "    .last()\n",
    ")\n",
    "contiguous_usa[\"abbr\"] = contiguous_usa.state.apply(STATE_TO_ABBR.__getitem__)\n",
    "contiguous_usa[\"total_covid_cases\"] = contiguous_usa.abbr.apply(\n",
    "    lambda a: total_cases.loc[a][\"tot_cases\"]\n",
    ")\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    cluster_df_clean,\n",
    "    geometry=gpd.points_from_xy(cluster_df_clean.lng, cluster_df_clean.lat),\n",
    ")\n",
    "\n",
    "usa_gdf = gdf.loc[gdf.within(usa.iloc[0][\"geometry\"])].copy()\n",
    "\n",
    "cluster_cnts = [0 for _ in range(len(contiguous_usa))]\n",
    "covid_cnts = [0 for _ in range(len(contiguous_usa))]\n",
    "for idx, row in usa_gdf.iterrows():\n",
    "    state_match = None\n",
    "    for state_idx, state in contiguous_usa.iterrows():\n",
    "        if state[\"geometry\"].contains(Point(row[\"lng\"], row[\"lat\"])):\n",
    "            state_match = state_idx\n",
    "            break\n",
    "    if state_match is None:\n",
    "        continue\n",
    "    cluster_cnts[state_match] += 1\n",
    "    covid_cnts[state_match] += row[\"covid_cnt\"]\n",
    "contiguous_usa[\"cluster_cnts\"] = cluster_cnts\n",
    "contiguous_usa[\"covid_cnts\"] = covid_cnts\n",
    "contiguous_usa[\"mentions_per_cluster\"] = (\n",
    "    contiguous_usa[\"covid_cnts\"] / contiguous_usa[\"cluster_cnts\"]\n",
    ")\n",
    "contiguous_usa[\"cases_per_population\"] = (\n",
    "    contiguous_usa[\"total_covid_cases\"] / contiguous_usa[\"population\"]\n",
    ")\n",
    "\n",
    "print(round(len(usa_gdf) / len(gdf) * 100), \"% in USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gplt.webmap(contiguous_usa, projection=gcrs.WebMercator(), figsize=(16, 16))\n",
    "gplt.pointplot(usa_gdf, ax=ax).set_title(\"Venmo Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gplt.choropleth(\n",
    "    contiguous_usa,\n",
    "    hue=\"covid_cnts\",\n",
    "    projection=gcrs.AlbersEqualArea(),\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    cmap=\"Reds\",\n",
    "    legend=True,\n",
    "    scheme=\"FisherJenks\",\n",
    ").set_title(\"COVID Mentions\")\n",
    "gplt.choropleth(\n",
    "    contiguous_usa,\n",
    "    hue=\"cluster_cnts\",\n",
    "    projection=gcrs.AlbersEqualArea(),\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    cmap=\"Blues\",\n",
    "    legend=True,\n",
    "    scheme=\"FisherJenks\",\n",
    ").set_title(\"Venmo Clusters\")\n",
    "gplt.choropleth(\n",
    "    contiguous_usa,\n",
    "    hue=\"mentions_per_cluster\",\n",
    "    projection=gcrs.AlbersEqualArea(),\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    cmap=\"Purples\",\n",
    "    legend=True,\n",
    "    scheme=\"FisherJenks\",\n",
    ").set_title(\"COVID Mentions Per Venmo Cluster\")\n",
    "gplt.choropleth(\n",
    "    contiguous_usa,\n",
    "    hue=\"cases_per_population\",\n",
    "    projection=gcrs.AlbersEqualArea(),\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    "    cmap=\"Greys\",\n",
    "    legend=True,\n",
    "    scheme=\"FisherJenks\",\n",
    ").set_title(\"Cases Per Population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
